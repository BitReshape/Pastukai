{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK0HUdnEO7S4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29BUYhaKO9SQ"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"/content/drive/MyDrive/uni/pastukai/temp\"\n",
    "!mkdir -p \"/content/drive/MyDrive/uni/pastukai/data\"\n",
    "!ls \"/content/drive/MyDrive/uni/pastukai\"\n",
    "%cd \"/content/drive/MyDrive/uni/pastukai/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DAR3dz3O_pJ"
   },
   "outputs": [],
   "source": [
    "# Downloading testing set\n",
    "\n",
    "%%bash\n",
    "\n",
    "DATASET_DIR=\"/content/drive/MyDrive/uni/pastukai/temp/dataset/\"\n",
    "\n",
    "if [ ! -f \"ISIC_2019_Test_Input.zip\" ]; then\n",
    "    echo \"Downloading Training Data ...\"\n",
    "    wget --show-progress --progress=bar:force https://isic-challenge-data.s3.amazonaws.com/2019/ISIC_2019_Test_Input.zip -O ISIC_2019_Test_Input.zip\n",
    "fi\n",
    "\n",
    "echo \"Unpacking ISIC_2019_Test_Input.zip ...\"\n",
    "unzip -q -j ISIC_2019_Test_Input.zip -d $DATASET_DIR\n",
    "\n",
    "# Number of files in dataset folder.\n",
    "ls $DATASET_DIR | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsYLwqSMPlJZ"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_DIR=\"/content/drive/MyDrive/uni/pastukai/temp/dataset/\"\n",
    "\n",
    "if [ -d $DATASET_DIR ] && [ $(ls -1 $DATASET_DIR | wc -l) -eq 25333 ]; then\n",
    "    echo \"Successfully built the dataset\"\n",
    "else\n",
    "    echo \"Error when building the dataset\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg3Fl2NuQGG7"
   },
   "source": [
    "**Preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SEF9Y-DTPr5i"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import exposure, morphology, filters, img_as_ubyte, img_as_float\n",
    "from skimage.color.adapt_rgb import adapt_rgb, each_channel\n",
    "\n",
    "def enlarge_image(img):\n",
    "    '''\n",
    "    Enlarge image with dark border areas\n",
    "    :param img: ndarray, BGR image\n",
    "    :return: ndarray, BGR image\n",
    "    '''\n",
    "    f = np.zeros((700, 700,3), np.uint8)\n",
    "    ax, ay = (700 - img.shape[1]) // 2, (700 - img.shape[0]) // 2\n",
    "    f[ay:img.shape[0] + ay, ax:ax + img.shape[1]] = img\n",
    "    return f\n",
    "\n",
    "\n",
    "def reduce_image(img):\n",
    "    '''\n",
    "    Resize image to size 600x600 and remove dark border areas\n",
    "    :param img: ndarray, BGR image\n",
    "    :return: ndarray, BGR image\n",
    "    '''\n",
    "    ax, ay = (img.shape[1]-600)//2, (img.shape[0] - 600) // 2\n",
    "    f = img[ay:600 + ay, ax:ax + 600]\n",
    "    return f\n",
    "\n",
    "\n",
    "def reduce_mask(img):\n",
    "    '''\n",
    "    Reuce mask size to 600x600\n",
    "    :param img: ndarray, binary image\n",
    "    :return: ndarray, binary image\n",
    "    '''\n",
    "    ax, ay = (img.shape[1]-600)//2, (img.shape[0] - 600) // 2\n",
    "    f = img[ay:600 + ay, ax:ax + 600]\n",
    "    return f\n",
    "\n",
    "\n",
    "def remove_black_border(gray_image):\n",
    "    '''\n",
    "    Remove black border areas\n",
    "    :param gray_image: ndarray, grayscale image\n",
    "    :return: ndarray, binary image\n",
    "    '''\n",
    "    _, mask = cv2.threshold(gray_image, 10, 255, cv2.THRESH_BINARY);\n",
    "    (contours, _) = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    mask_a = np.zeros((700, 700), np.uint8)\n",
    "    cv2.drawContours(mask_a, [c], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    kernel = np.ones((15, 15), np.uint8)\n",
    "    eroded = cv2.erode(mask_a, kernel, iterations=3)\n",
    "    _, mask_a = cv2.threshold(eroded, 10, 255, cv2.THRESH_BINARY);\n",
    "    return mask_a\n",
    "\n",
    "\n",
    "@adapt_rgb(each_channel)\n",
    "def morph_closing_each(image, struct_element):\n",
    "    return morphology.closing(image, struct_element)\n",
    "\n",
    "\n",
    "@adapt_rgb(each_channel)\n",
    "def median_filter_each(image, struct_element):\n",
    "    return filters.median(image, struct_element)\n",
    "\n",
    "\n",
    "structuring_element = morphology.disk(7)\n",
    "\n",
    "\n",
    "def crop_center_rgb(img, cropx, cropy):\n",
    "    '''\n",
    "    Crop image from the center\n",
    "    :param img: ndarray, BGR image\n",
    "    :param cropx: width in int\n",
    "    :param cropy: height in int\n",
    "    :return: ndarray, BGR cropped image\n",
    "    '''\n",
    "    y,x,_ = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)\n",
    "    return img[starty:starty+cropy,startx:startx+cropx,:]\n",
    "\n",
    "\n",
    "def noise_removal(img):\n",
    "    '''\n",
    "    Remove noise in the image\n",
    "    :param img: ndarray, BGR image\n",
    "    :return: ndarray, BGR filtered image\n",
    "    '''\n",
    "    img = img_as_float(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    equalized_adapthist = exposure.equalize_adapthist(img)\n",
    "    img_morph_closing = morph_closing_each(equalized_adapthist, structuring_element)\n",
    "    img_filtered = median_filter_each(img_morph_closing, structuring_element)\n",
    "    img_filtered = cv2.cvtColor(img_as_ubyte(img_filtered), cv2.COLOR_RGB2BGR)\n",
    "    return img_filtered\n",
    "\n",
    "\n",
    "# Not used anymore\n",
    "# See https://www.kaggle.com/apacheco/shades-of-gray-color-constancy\n",
    "def shade_of_gray_cc(img, power=6, gamma=None):\n",
    "    \"\"\"\n",
    "    img (numpy array): the original image with format of (h, w, c)\n",
    "    power (int): the degree of norm, 6 is used in reference paper\n",
    "    gamma (float): the value of gamma correction, 2.2 is used in reference paper\n",
    "    \"\"\"\n",
    "    img_dtype = img.dtype\n",
    "\n",
    "    if gamma is not None:\n",
    "        img = img.astype('uint8')\n",
    "        look_up_table = np.ones((256, 1), dtype='uint8') * 0\n",
    "        for i in range(256):\n",
    "            look_up_table[i][0] = 255 * pow(i / 255, 1 / gamma)\n",
    "        img = cv2.LUT(img, look_up_table)\n",
    "\n",
    "    img = img.astype('float32')\n",
    "    img_power = np.power(img, power)\n",
    "    rgb_vec = np.power(np.mean(img_power, (0, 1)), 1 / power)\n",
    "    rgb_norm = np.sqrt(np.sum(np.power(rgb_vec, 2.0)))\n",
    "    rgb_vec = rgb_vec / rgb_norm\n",
    "    rgb_vec = 1 / (rgb_vec * np.sqrt(3))\n",
    "    img = np.multiply(img, rgb_vec)\n",
    "\n",
    "    # Andrew Anikin suggestion\n",
    "    img = np.clip(img, a_min=0, a_max=255)\n",
    "\n",
    "    return img.astype(img_dtype)\n",
    "\n",
    "\n",
    "def preprocess_image(img):\n",
    "    '''\n",
    "    Preprocess image\n",
    "    :param img: ndarray, BGR image\n",
    "    :return:  ndarray, BGR image\n",
    "    '''\n",
    "    img = noise_removal(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def crop_image(img):\n",
    "    '''\n",
    "    Remove dark border areas and crop image\n",
    "    :param img: ndarray, BGR image\n",
    "    :return: ndarray, BGR image\n",
    "    '''\n",
    "    img = cv2.resize(img, (600, 600))\n",
    "    inpaint_image = enlarge_image(img)\n",
    "\n",
    "    # Remove black border\n",
    "    gray = cv2.cvtColor(inpaint_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    mask = remove_black_border(gray)\n",
    "    #mean = cv2.mean(inpaint_image, mask)\n",
    "    mask = reduce_mask(mask)\n",
    "    inpaint_image = reduce_image(inpaint_image)\n",
    "    inpaint_image[mask == 0] = 0\n",
    "\n",
    "    coords = cv2.findNonZero(mask)\n",
    "    x, y, w, h = cv2.boundingRect(coords)\n",
    "    inpaint_image = inpaint_image[y:y + h, x:x + w]\n",
    "    inpaint_image = cv2.resize(inpaint_image, (600, 600))\n",
    "    # inpaint_image = shade_of_gray_cc(inpaint_image)\n",
    "\n",
    "    # Crop from the center the image if the borders are black\n",
    "    gray_img = cv2.cvtColor(inpaint_image, cv2.COLOR_BGR2GRAY)\n",
    "    if gray_img[0][0] < 10 and gray_img[0][-1] < 10 and gray_img[-1][0] < 10 and gray_img[-1][-1] < 10:\n",
    "        inpaint_image = cv2.resize(crop_center_rgb(inpaint_image, 400, 400), (600,600))\n",
    "    return inpaint_image\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_path, dataset_csv_path, preprocessed_dataset_path):\n",
    "    '''\n",
    "    Preprocess training and validation dataset\n",
    "    :param dataset_path: Path to dataset\n",
    "    :param dataset_csv_path: Path to training csv file\n",
    "    :param val_csv_path: Path to validation csv file\n",
    "    :param preprocessed_dataset_path: Path where to save preprocessed  images\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    df_train = pd.read_csv(dataset_csv_path)\n",
    "\n",
    "    print('Start of preprocessing step of dataset ')\n",
    "    for i, image in df_train.iterrows():\n",
    "        img = cv2.imread(dataset_path + image['image']+'.jpg', cv2.IMREAD_COLOR)\n",
    "        img = preprocess_image(img)\n",
    "        cv2.imwrite(preprocessed_dataset_path + image['image']+'.jpg', img)\n",
    "        print(str(i) + ': Preprocessed image ' + image['image'])\n",
    "    print('Finished preprocess step of dataset')\n",
    "    print('preprocessed images saved in ' + preprocessed_dataset_path)\n",
    "    print('Finished script')\n",
    "\n",
    "\n",
    "def crop_dataset(dataset_path, dataset_csv_path):\n",
    "    '''\n",
    "    Crop training and validation dataset\n",
    "    :param dataset_path: Path to training dataset\n",
    "    :param dataset_csv_path: Path to training csv file\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    df_dataset = pd.read_csv(dataset_csv_path)\n",
    "\n",
    "    print('Start of dataset cropping step')\n",
    "    for i, image in df_dataset.iterrows():\n",
    "        img = cv2.imread(dataset_path + image['image']+'.jpg', cv2.IMREAD_COLOR)\n",
    "        img = crop_image(img)\n",
    "        cv2.imwrite(dataset_path + image['image']+'.jpg', img)\n",
    "        print(str(i) + ': Cropped image ' + image['image'])\n",
    "    print('Finished cropping step of dataset')\n",
    "    print('cropped  images saved in ' + dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5SB0Gw5Rz8W"
   },
   "source": [
    "**Create CSV file for testing images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hzCRaahbRzeE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# create CSV file of all images \n",
    "def create_testing_file_csv(datatset_path, testing_set_csv_path):\n",
    "    '''\n",
    "    Function to create csv file out of testing dataset\n",
    "    :param datatset_path: Path of the testing dataset\n",
    "    :param testing_set_csv_path: Path to the testing csv file\n",
    "    :return:\n",
    "    '''\n",
    "    filenames = []\n",
    "    \n",
    "    # Get all image filenames\n",
    "    for filename in os.listdir(datatset_path):\n",
    "        filenames.append(filename[:-4])\n",
    "        \n",
    "    # save image filenames in csv\n",
    "    csvfilenames = []\n",
    "    with open(testing_set_csv_path + 'testing.csv','w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image'])\n",
    "        for file in filenames:\n",
    "            if file[0:4] == \"ISIC\":\n",
    "                writer.writerow([file])\n",
    "                csvfilenames.append(file)     \n",
    "    print(\"CSV file is created successfully.\")\n",
    "    \n",
    "\n",
    "def create_results_file_csv(results_path, csv_name, images, predictions, max_predictions):\n",
    "    '''\n",
    "    :param results_path: Path to the results directory\n",
    "    :param csv_name: Name of the csv file\n",
    "    :param images: array of image names\n",
    "    :param predictions: Array of predicted class\n",
    "    :param max_predictions: Array with probability of the predictions\n",
    "    :return:\n",
    "    '''\n",
    "    # check length of both files if they are equal\n",
    "    if len(images) != len(predictions) or len(predictions) != len(max_predictions):\n",
    "        print(\"!!!!! Length is not the same of the image array and prediction array !!!!!    image lenght = \", len(images) , \" prediction length = \", len(predictions), \" max prediction length = \", len(max_predictions))\n",
    "        \n",
    "    # save imagesnames and predictions into csv file\n",
    "    with open(results_path + csv_name + '.csv','w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # {'AK': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, 'NV': 5, 'SCC': 6, 'VASC': 7}\n",
    "        # {'AK': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, 'NV': 5, 'SCC': 6, 'VASC': 7}  \n",
    "        writer.writerow(['image', 'AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC', 'UNK'])\n",
    "        print('hier:', predictions)\n",
    "        for i in range(len(images)):\n",
    "            if max_predictions[i] < 0.25:\n",
    "                writer.writerow([images[i],0,0,0,0,0,0,0,0,1])\n",
    "            else:\n",
    "                if predictions[i] == 0:\n",
    "                    writer.writerow([images[i],1,0,0,0,0,0,0,0,0])\n",
    "                elif predictions[i] == 1:\n",
    "                    writer.writerow([images[i],0,1,0,0,0,0,0,0,0])\n",
    "                elif predictions[i] == 2:\n",
    "                    writer.writerow([images[i],0,0,1,0,0,0,0,0,0])\n",
    "                elif predictions[i] == 3:\n",
    "                    writer.writerow([images[i],0,0,0,1,0,0,0,0,0])            \n",
    "                elif predictions[i] == 4:\n",
    "                    writer.writerow([images[i],0,0,0,0,1,0,0,0,0])\n",
    "                elif predictions[i] == 5:\n",
    "                    writer.writerow([images[i],0,0,0,0,0,1,0,0,0])\n",
    "                elif predictions[i] == 6:\n",
    "                    writer.writerow([images[i],0,0,0,0,0,0,1,0,0])\n",
    "                elif predictions[i] == 7:\n",
    "                    writer.writerow([images[i],0,0,0,0,0,0,0,1,0])\n",
    "                elif predictions[i] == 8:\n",
    "                    writer.writerow([images[i],0,0,0,0,0,0,0,0,1])\n",
    "                else:\n",
    "                    print('Error! this class is unknown! Number:', i, 'Prediction:', predictions[i], 'images:', images[i])\n",
    "\n",
    "    print(\"CSV file is created successfully.\")\n",
    "                 \n",
    "    \n",
    "    \n",
    "def getImageTestingNames(testing_set_csv_path):\n",
    "    '''\n",
    "    Get all testing images name\n",
    "    :param testing_set_csv_path:\n",
    "    :return: Array of image names\n",
    "    '''\n",
    "    df_testing = pd.read_csv(testing_set_csv_path)\n",
    "    image_names = []\n",
    "    \n",
    "    for i, image in df_testing.iterrows():\n",
    "        image_names.append(image['image'])\n",
    "    \n",
    "    return image_names\n",
    "\n",
    "\n",
    "\n",
    "def getMaxPredictions(predicted_testing_prob):\n",
    "    '''\n",
    "    Get from the svm output the maximum probability for each image\n",
    "    :param predicted_testing_prob: Probabilities for each class of every image\n",
    "    :return: Array of max probabilities\n",
    "    '''\n",
    "    max_prediction = []\n",
    "    for item in predicted_testing_prob:\n",
    "        max_prediction.append(max(item))\n",
    "    return max_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXm-r4WVQ-MZ"
   },
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "givzdZ7hQ9vL"
   },
   "outputs": [],
   "source": [
    "# Set paths\n",
    "dataset_path = '/content/drive/MyDrive/uni/pastukai/temp/dataset/'\n",
    "testing_csv = '/content/drive/MyDrive/uni/pastukai/data/testing.csv'\n",
    "testing_csv_path = '/content/drive/MyDrive/uni/pastukai/data/'\n",
    "result_csv_path = '/content/drive/MyDrive/uni/pastukai/data/result.csv'\n",
    "\n",
    "model_h5_path = \"/content/drive/MyDrive/uni/pastukai/model.h5\"\n",
    "\n",
    "class_labels = ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']\n",
    "\n",
    "NUM_CLASSES = 8\n",
    "IMG_SIZE = 160\n",
    "dropout_rate = 0.4\n",
    "batch_size = 20\n",
    "epochs = 50\n",
    "print('Build directory structure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEl7m-ldSbkN"
   },
   "outputs": [],
   "source": [
    "# create CSV file for images\n",
    "create_testing_file_csv(dataset_path, testing_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrHj8STAQ8cL"
   },
   "outputs": [],
   "source": [
    "# Skip this step if you intend to use the last dataset split\n",
    "# Remove black border from validation  images \n",
    "crop_dataset(dataset_path, testing_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uoDVewFmTo8E"
   },
   "outputs": [],
   "source": [
    "# read csv file for testing\n",
    "\n",
    "def read_csv_files(testing_csv):\n",
    "    '''\n",
    "    Read csv files for training and validation\n",
    "    :param train_csv: Path to train csv file\n",
    "    :param val_csv: Path to val csv file\n",
    "    :return: dict train_classes (key: image_name, value: class), dict val_classes (key: image_name, value: class)\n",
    "    '''\n",
    "    testing_classes = {}\n",
    "    df_testing = pd.read_csv(testing_csv)\n",
    "    for i, image in df_testing.iterrows():\n",
    "         testing_classes[image['image']] = \"unkown class\"\n",
    "\n",
    "    return testing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ifrw5m0jT-d2"
   },
   "outputs": [],
   "source": [
    "# Read dataframes\n",
    "testing_df = read_csv_files(testing_csv)\n",
    "\n",
    "imageNames = []\n",
    "new_testing = {}\n",
    "\n",
    "for key,value in testing_df.items():\n",
    "  new_testing[key+'.jpg'] = value \n",
    "  imageNames.append(key)\n",
    "\n",
    "testing_df = new_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmVfbyEBVQvP"
   },
   "outputs": [],
   "source": [
    "testing_df = pd.DataFrame(list(testing_df.items()), columns=['image_name','class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Jn-wwFDVYFn"
   },
   "source": [
    "**Build, load and run model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cujP_yKgVX3S"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras import optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense,GlobalAveragePooling2D,Flatten,Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D,Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_model_mobilenet(num_classes):\n",
    "  base_model=MobileNet(weights='imagenet',include_top=False,input_shape=(IMG_SIZE, IMG_SIZE, 3)) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "  x=base_model.output\n",
    "  x=GlobalAveragePooling2D()(x)\n",
    "  x=Dropout(0.4)(x)\n",
    "\n",
    "  x=Dense(300,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better result\n",
    "  x=Dropout(0.4)(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  x=Dense(100,activation='relu')(x) #dense layer 2\n",
    "  x=Dropout(0.4)(x)\n",
    "\n",
    "  x=Dense(50,activation='relu')(x) #dense layer 3\n",
    "  preds=Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "  model=Model(inputs=base_model.input,outputs=preds)\n",
    "  print(len(model.layers[:]))\n",
    "  for layer in model.layers[:85]:\n",
    "    layer.trainable=False\n",
    "  for layer in model.layers[85:]:\n",
    "    layer.trainable=True\n",
    "  model.summary()\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.1),\n",
    "              metrics=['acc'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk0kA-bOWCyi"
   },
   "outputs": [],
   "source": [
    "# build and load model\n",
    "\n",
    "model = build_model_mobilenet(num_classes=NUM_CLASSES)\n",
    "model.load_weights(model_h5_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YFIvUu9X6Dl"
   },
   "source": [
    "**Predict data with model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeHMVQ4UWyHf"
   },
   "outputs": [],
   "source": [
    "# Prediction of testing data\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "testing_iterator = datagen.flow_from_dataframe(dataframe=testing_df,\n",
    "                                                directory=dataset_path,\n",
    "                                                x_col=\"image_name\",\n",
    "                                                y_col=None,\n",
    "                                                batch_size=batch_size,\n",
    "                                                seed=42,\n",
    "                                                shuffle=False,\n",
    "                                                class_mode=None,\n",
    "                                                target_size=(IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jKu98bdXpfG"
   },
   "outputs": [],
   "source": [
    "# predict testing data with the model\n",
    "testing_iterator.reset()\n",
    "pred=model.predict_generator(testing_iterator,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Y7OWunv02s6"
   },
   "outputs": [],
   "source": [
    "# save prediction in csv file\n",
    "predicted_class_indices=np.argmax(pred,axis=1)\n",
    "create_results_file_csv(testing_csv_path,'results', imageNames, predicted_class_indices, getMaxPredictions(pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
